{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7879a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077886f",
   "metadata": {},
   "source": [
    "# 3. Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65624ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAINING_PICTURES = \"../data/fruits-360_3-body-problem/Training\"\n",
    "PATH_TEST_PICTURES = \"../data/fruits-360_3-body-problem/Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77408d8",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b7a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b69be711",
   "metadata": {},
   "source": [
    "# 5. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defbecb",
   "metadata": {},
   "source": [
    "## 5.1 Data Loading and Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fddb5",
   "metadata": {},
   "source": [
    "We are going to transform the pictures to be all of a certain size. And we are also then going to apply in the same pipeline the standardization (in the ToTensor) of the picture to transform it into the right format for the neural network. Lastly, we also apply a normalization of the pictures. So that we can have the data between -1 and 1 so that the training of the neural network can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "795f652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f60c35",
   "metadata": {},
   "source": [
    "We are going to use ImageFolder to apply a pipeline transformation for each of the train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e774da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageFolder(root=PATH_TRAINING_PICTURES, transform=transform)\n",
    "test_data = ImageFolder(root=PATH_TEST_PICTURES, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb4928",
   "metadata": {},
   "source": [
    "We are going to do a DataLoader. It's a function that will inform the model what the train and test data are. Along with what the batch sizes are going to be and whether shuffling will take place or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a674f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True) # I want to shuffle all the pictures when training (ie shuffle the pears with apples etc)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "465a15ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'Cherry', 'Tomatoe']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825fcf0",
   "metadata": {},
   "source": [
    "## 5.2 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ce257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDetector(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FruitDetector, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,16,kernel_size=3, stride=1, padding=1) \n",
    "        # Conv2d: kernel will be of 3x3 dimensions, parameters: 3 channels, characteristical maps: 16, kernel size: 3x3, stride: 1\n",
    "        self.conv2=nn.Conv2d(16,32,kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1=nn.Linear(32*25*25,128) \n",
    "        # images of 25*25 and each image has 32 characteristical maps.\n",
    "        # 25: we go from 100, then /2 because of the first max pool, then we apply the same for the second conv nn and we end up with 25. \n",
    "        # 32: are the characteristical maps that come from the conv1. 128: will be the number of output neurons\n",
    "        self.fc2=nn.Linear(128,num_classes)\n",
    "    \n",
    "    def forward(self, x): # every batch (x) will follow this treatment. [32, 3, 100, 100] -> each x has batches of 32, 3 channels, size=100*100 \n",
    "        x=F.relu(self.conv1(x)) # activation function. End result [32, 16, 100, 100]\n",
    "        x=F.max_pool2d(x,2) # to better generalize in the training and try to avoid overfitting. End result [32, 16, 50, 50]\n",
    "        x=F.relu(self.conv2(x)) # End result [32, 32, 50, 50]\n",
    "        x=F.max_pool2d(x,2) # End result [32, 32, 25, 25]\n",
    "        x=torch.flatten(x,1) # End result [32, 32*25*25]\n",
    "        x=F.relu(self.fc1(x)) # End result [32, 128]\n",
    "        x=self.fc2(x) # End result [32, num_classes]\n",
    "        return(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "392e76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Select the device on which to run the model. By default it's a cpu, although if there's an open cuda (a more powerful cpu from NVidia) it will use that\n",
    "\n",
    "model=FruitDetector(num_classes=len(train_data.classes)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152421dd",
   "metadata": {},
   "source": [
    "## 5.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b47679",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Loss function for multi-class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam is a type of optimizer. It's going to update the parameters with the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "142a24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0003\n",
      "Epoch 2, Loss: 0.0001\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000\n",
      "Epoch 6, Loss: 0.0000\n",
      "Epoch 7, Loss: 0.0000\n",
      "Epoch 8, Loss: 0.0000\n",
      "Epoch 9, Loss: 0.0000\n",
      "Epoch 10, Loss: 0.0000\n",
      "Epoch 11, Loss: 0.0000\n",
      "Epoch 12, Loss: 0.0000\n",
      "Epoch 13, Loss: 0.0000\n",
      "Epoch 14, Loss: 0.0000\n",
      "Epoch 15, Loss: 0.0000\n",
      "Epoch 16, Loss: 0.0000\n",
      "Epoch 17, Loss: 0.0000\n",
      "Epoch 18, Loss: 0.0000\n",
      "Epoch 19, Loss: 0.0000\n",
      "Epoch 20, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    actual_loss=0\n",
    "    for image, label in train_loader:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(image)\n",
    "        loss=criterion(output, label)\n",
    "        loss.backward() # backward propagation\n",
    "        optimizer.step() # update the nn parameters\n",
    "        actual_loss=actual_loss+loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {actual_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068b42d",
   "metadata": {},
   "source": [
    "## 5.4 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e058c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy en test: {100*correct/total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
